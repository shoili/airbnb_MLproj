---
title: "AirBnB Pricing Predictions - ISyE 6740 Project"
date: "April 28, 2017"
output: pdf_document
---
  
## Team
  
Parit Burintrathikul - paritb@gatech.edu

Naveed Chowdhury - 

Shoili Pal - spal41@gatech.edu

Steven Yeh - syeh35@gatech.edu

## Introduction

The global hotel industry has been on the rise in the past two decades. With the rising middle class from across the globe, especially in places like China, people are travelling more than ever. The whole industry is currently estimated to be worth over 550 billion USD to grow to over 700 billion USD in 2021. Even in the US itself, it is valued at over 200 billion USD. 

AirBnB is a start-up based in San Francisco, CA. Their business model is that homeowners with spare rooms or vacant properties can rent out the property for any duration of time. As a company, AirBnB is growing at a rapid rate. In its last round of investment, it was able to raise over 850 million USD with a current valuation of over 30 Billion USD.

Our objective in this project is to create a model from airBnB leasing data to predict and estimate per night prices of new properties in Boston and understanding what main indicators help drive the price of a property. This could help potential new landlords evaluate their roomâ€™s worth and not under/overvalue their property. The models can probably be generalized for use in other cities too. 

## Data

### Data Source and Description

The data we are using is sourced from Kaggle, https://www.kaggle.com/airbnb/boston. It was scraped from airbnb.com. This dataset only contains information about AirBnb listings and transactions from Boston. The data was separated in three main components described below:

* Listings
    * Properties of the listing like amenities, neighborhood, number of rooms
    * Text fields like description of the property
* Reviews
    * Reviews of the place by people who have stayed there
* Calendar
    * When the property is available
    * How much the price is on that date

### Exploratory Data Analysis

#### Reviews

There were a total of 68275 reviews from the listings in Boston, split into 14 different languages. 93% of the reviews were primarily in English, 4% were split between French, Spanish and Turkish and the last 3% were comprised of the rest of the 10 languages. Since the majority of the reviews were in English, we decided to focus only on them and remove the non-English reviews. 

### Data Pre-processing

In our initial data preprocessing, we removed around 4-5 predictors which were homogenous across all the listings. We also removed fields like monthly price and weekly price which are basically a multiple of the nightly price which we are trying to predict. 

#### Creating the Final Dataset

The three main datasets are joined on listing ids to create a master dataset to train our algorithms on. The joined dataset had around 116 unique variables and 700k observations, due to the periodic scraping of listings over time to reflect change in price over time. We decided to model the average price per night of each listing since all our predictor variables stay the same except the date. We checked that the variance over time was not too high for most of the listings. 

## Feature Engineering

As mentioned earlier, there are some text fields in the data. These include reviews of the properties written by previous tenants as well as descriptions of the property and neighborhood written by the landlords. We implemented simple text mining techniques to generate additional features to be used in our model.

### Most Frequent Words



### Sentiment Analysis

Using the multitudes of reviews available to us, we converted each review to a sentiment polarity score, ranging between [-1,1] where 1 is a wholly positive review and -1 is a wholly negative review. We had decided against building our own specific corpus for sentiment analysis due to the lack of labelled review data; the reviews themselves only contained the user and his/her review text without review specific ratings. While the specifically trained model may provide us with more reflectively scores, the unavailability of such data convinced us to proceed with using pre-trained models. 


# Methods



## Models

## Linear Regression

  Simple linear regression was used as a baseline model for performance. As expected, fitting a linear regression on the entire dataset yielded unfavorable results, with an RMSE > 44.7. A possible reason for this performance was due to the number of highly priced rentals (>\$200/night). As a result, the dataset was further subsetted into three smaller components: listings under \$100 nightly, listings between \$100 and \$200 nightly, and listings over \$200 nightly. Regression models fit for listings over \$200 performed very poorly so we limited our investigation to the rest of the listings.
  
  Feature selection was also performed using the original linear regression model. The most "useful" features were selected by examining the p-values for each coefficient in the fitted model and picking only the features with a p-value below a certain threshold. For models that do not innately perform feature selection, the p-value threshold was set at 10%. This reduced the amount of features down to 29 most statistically significant features.

  Each binned dataset was further split into 80-20 training-testing data sets for validation purposes. This split was chosen due to the limited number of listings in Boston. Through 100 iterations, each model was trained onto the training set and predictions were generated and compared to the testing set. The comparison was reflected via root mean squared error (RMSE). The 100 RMSE values were then averaged and compared across models to determine the best performing model. A paired samples t-test was used to determine whether the best model performed statistically significantly better than the other models. 
  
## Stepwise Regression 
  
  Stepwise regression (MASS library) was performed using the previously fitted linear model. Using bidirectional selection, both forward and backward elimination were used to eliminate or add variables at every step. 
  
## Lasso Regression

  Since Lasso Regression (lars library) performs feature selection, a wider number of features can be included while fitting the model. Therefore, the p-value threshold was shifted up to 30%, which increased the amount of features to 51. Lasso regression forces the sum of the absolute value of the regression coefficients to be less than a fixed value, which serves as a feature selection procedure. 

## Ridge Regression

  Ridge regression (lm.ridge) does not perform feature selection, though it does force the sum of squares of the regression coefficients to be less than a fixed value. Ridge regression does provide an improved prediction by shrinking coefficients that are large in magnitude to prevent overfitting. Ridge regression uses a lambda parameter to minimize mean squared error, which was optimized through generalized cross validation (GCV). 
  
## KNN Regression

  KNN Regression (FNN library) is an application of the K nearest neighbors algorithm to estimate continuous variables. Several values of k were chosen: k=1, 3, 5, 7, 15. For both the \$100/night and \$200/night binned datasets, the best k-value was 15. In other words, the algorithm would take the 15 closest neighbors (Euclidean distance) and generated a prediction using an inverse distance weighted average for those 15 neighbors. 

## CART: Regression Tree

  Regression trees (rpart library) uses decision rules to predict a continuous outcome. A cost complexity factor of 0.0001 was used to dictate that a split must decrease the overall lack of fit by a factor of 0.0001 before being performed. 

## Random Forest 

# Results


For Binary Classification

Name    | Mean Error | Error Variance
------------- | ------------- 
  Bradley Terry Model | 0.536428 | 9.081612e-04
Logistic Regression (with Betting data) | 0.6342500 | 7.258291e-04
Logistic Regression (without Betting data) | 0.6235833 | 8.791171e-04
SVM (with Betting data) | 0.5041667 | 0
SVM (without Betting data) | 0.6447500 | 7.765094e-04
Random Forest (with Betting data) | 0.5263779 | 4.532934e-05
Random Forest (without Betting data) | 0.5325261  | 4.942625e-05


For 3-Class Classification

Name | Testing Error
------------- | ------------- 
  Multinomial Regression (with Betting data) | 0.5005000 | 9.110261e-04
Multinomial Regression (without Betting data) | 0.4871667 | 8.608560e-04
SVM (with Betting data) | 0.5065833 | 5.768778e-04
SVM (without Betting data) | 0.5046667 | 6.098639e-04
Random Forest (with Betting data) | 0.5277557 | 5.372419e-05
Random Forest (without Betting data) | 0.5095616 | 5.750444e-05
Ensemble Method (with Betting data) | 0.4471786 | 3.456211e-05
Ensemble Method (without Betting data) | 0.4614107 | 3.144835e-05

### Statistical Tests



## Discussion



## Conclusions




## Bibliography and Credits

* Boston AirBnB Data

https://www.kaggle.com/airbnb/boston

* R 

https://www.r-project.org/
  
https://cran.r-project.org/doc/manuals/r-patched/R-intro.pdf

  